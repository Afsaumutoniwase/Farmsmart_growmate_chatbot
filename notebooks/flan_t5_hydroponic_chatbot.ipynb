{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76f5a2f0",
   "metadata": {},
   "source": [
    "# GrowMate: FLAN-T5 Hydroponic Chatbot\n",
    "\n",
    "This notebook creates an advanced hydroponic chatbot using Google's FLAN-T5-base model. FLAN-T5 is fine-tuned for instruction following, making it ideal for conversational AI applications.\n",
    "\n",
    "## Features:\n",
    "- **FLAN-T5-base**: More powerful than T5-small with better instruction following\n",
    "- **Hydroponic Domain**: Specialized for hydroponic farming questions\n",
    "- **Conversational**: Natural dialogue capabilities\n",
    "- **Rwanda Context**: Tailored for local farming conditions\n",
    "\n",
    "## Workflow:\n",
    "1. **Setup & Data Loading** - Load hydroponic FAQ data\n",
    "2. **FLAN-T5 Model Setup** - Configure the instruction-tuned model\n",
    "3. **Data Preprocessing** - Format data for instruction tuning\n",
    "4. **Fine-tuning** - Train on hydroponic domain\n",
    "5. **Evaluation & Testing** - Validate performance\n",
    "6. **Deployment Prep** - Save model for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5247e36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "✓ transformers>=4.25.0\n",
      "✓ transformers>=4.25.0\n",
      "✓ torch\n",
      "✓ torch\n",
      "✓ datasets\n",
      "✓ datasets\n",
      "✓ accelerate\n",
      "✓ accelerate\n",
      "✓ rouge-score\n",
      "✓ rouge-score\n",
      "✓ evaluate\n",
      "✓ evaluate\n",
      "✓ pandas\n",
      "✓ pandas\n",
      "✓ numpy\n",
      "✓ numpy\n",
      "✓ scikit-learn\n",
      "✓ scikit-learn\n",
      "✓ nltk\n",
      "\n",
      "Package installation completed!\n",
      "✓ nltk\n",
      "\n",
      "Package installation completed!\n"
     ]
    }
   ],
   "source": [
    "# Install Required Packages\n",
    "import subprocess\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "def install_package(package: str) -> None:\n",
    "    \"\"\"Install a package using pip.\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        print(f\"✓ {package}\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"✗ Failed to install {package}\")\n",
    "\n",
    "# Required packages with specific versions for compatibility\n",
    "REQUIRED_PACKAGES: List[str] = [\n",
    "    \"transformers>=4.25.0\",\n",
    "    \"torch\",\n",
    "    \"datasets\",\n",
    "    \"accelerate\",\n",
    "    \"rouge-score\", \n",
    "    \"evaluate\",\n",
    "    \"pandas\",\n",
    "    \"numpy\",\n",
    "    \"scikit-learn\",\n",
    "    \"nltk\"\n",
    "]\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "for package in REQUIRED_PACKAGES:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nPackage installation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf32305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Device: cpu\n",
      "PyTorch: 2.8.0+cpu\n",
      "\n",
      "Directories:\n",
      "   Base: c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\n",
      "   Data: c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\data\n",
      "   Model: c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\trained_model\n",
      "Device: cpu\n",
      "PyTorch: 2.8.0+cpu\n",
      "\n",
      "Directories:\n",
      "   Base: c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\n",
      "   Data: c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\data\n",
      "   Model: c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\trained_model\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    T5Tokenizer, \n",
    "    T5ForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Configure warnings and display\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "\n",
    "# Directory setup\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "MODEL_DIR = BASE_DIR / 'trained_model'\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\nDirectories:\")\n",
    "print(f\"   Base: {BASE_DIR}\")\n",
    "print(f\"   Data: {DATA_DIR}\")\n",
    "print(f\"   Model: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf7699",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Hydroponic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525906c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "   Samples: 625\n",
      "   Columns: ['question', 'answer']\n",
      "\n",
      "Data Quality:\n",
      "   Valid questions: 625 (100.0%)\n",
      "   Valid answers: 625 (100.0%)\n",
      "   Missing values: 0\n",
      "\n",
      "Sample Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What beginner mistakes should I avoid?</td>\n",
       "      <td>Overfeeding low dissolved oxygen poor sanitati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I keep records effectively?</td>\n",
       "      <td>Use a daily log for pH; EC; water temp; air te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How often should I calibrate meters?</td>\n",
       "      <td>Calibrate pH monthly and EC/TDS quarterly or a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 question  \\\n",
       "0  What beginner mistakes should I avoid?   \n",
       "1      How do I keep records effectively?   \n",
       "2    How often should I calibrate meters?   \n",
       "\n",
       "                                              answer  \n",
       "0  Overfeeding low dissolved oxygen poor sanitati...  \n",
       "1  Use a daily log for pH; EC; water temp; air te...  \n",
       "2  Calibrate pH monthly and EC/TDS quarterly or a...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and Explore Hydroponic Data\n",
    "def load_hydroponic_data(data_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load and validate hydroponic FAQ data.\"\"\"\n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"Data file not found: {data_path}\")\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Validate required columns\n",
    "    required_columns = ['question', 'answer']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "data_file = DATA_DIR / 'hydroponic_FAQS.csv'\n",
    "df = load_hydroponic_data(data_file)\n",
    "\n",
    "print(f\"Dataset Overview:\")\n",
    "print(f\"   Samples: {len(df):,}\")\n",
    "print(f\"   Columns: {list(df.columns)}\")\n",
    "\n",
    "# Data quality assessment\n",
    "valid_questions = df['question'].notna().sum()\n",
    "valid_answers = df['answer'].notna().sum()\n",
    "missing_values = df.isnull().sum().sum()\n",
    "\n",
    "print(f\"\\nData Quality:\")\n",
    "print(f\"   Valid questions: {valid_questions:,} ({valid_questions/len(df)*100:.1f}%)\")\n",
    "print(f\"   Valid answers: {valid_answers:,} ({valid_answers/len(df)*100:.1f}%)\")\n",
    "print(f\"   Missing values: {missing_values:,}\")\n",
    "\n",
    "print(f\"\\nSample Data:\")\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde3555",
   "metadata": {},
   "source": [
    "## 3. Load FLAN-T5-base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aff67997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/flan-t5-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Model parameters: 247,577,856\n",
      "Tokenizer vocab size: 32,100\n",
      "\n",
      "Model Test:\n",
      "   Question: What is the ideal pH for hydroponic lettuce?\n",
      "   Response: 6.5\n",
      "\n",
      "Model Test:\n",
      "   Question: What is the ideal pH for hydroponic lettuce?\n",
      "   Response: 6.5\n"
     ]
    }
   ],
   "source": [
    "# Load FLAN-T5-base Model and Tokenizer\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "\n",
    "def load_model_and_tokenizer(model_name: str) -> Tuple[T5ForConditionalGeneration, T5Tokenizer]:\n",
    "    \"\"\"Load FLAN-T5 model and tokenizer with optimal settings.\"\"\"\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Load model with appropriate dtype and device mapping\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32,\n",
    "        device_map=\"auto\" if device.type == 'cuda' else None\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def test_model(model: T5ForConditionalGeneration, tokenizer: T5Tokenizer, \n",
    "               test_question: str) -> str:\n",
    "    \"\"\"Test the model with a sample question.\"\"\"\n",
    "    input_text = f\"Answer this hydroponic farming question: {test_question}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=100,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = load_model_and_tokenizer(MODEL_NAME)\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer):,}\")\n",
    "\n",
    "# Test with sample question\n",
    "test_question = \"What is the ideal pH for hydroponic lettuce?\"\n",
    "response = test_model(model, tokenizer, test_question)\n",
    "\n",
    "print(f\"\\nModel Test:\")\n",
    "print(f\"   Question: {test_question}\")\n",
    "print(f\"   Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8a6295",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing for Instruction Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa0d9878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data...\n",
      "Filtered dataset: 625 samples (removed 0)\n",
      "Created 625 instruction-target pairs\n",
      "\n",
      "Sample Instruction:\n",
      "   Input: Answer this hydroponic farming question: What beginner mistakes should I avoid?\n",
      "   Target: Overfeeding low dissolved oxygen poor sanitation light leaks and skipping logs; start simple and scale.\n",
      "\n",
      "Length Statistics:\n",
      "   Average input: 11.8 words\n",
      "   Average target: 14.5 words\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing for Instruction Tuning\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text data.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove extra whitespace and line breaks\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def create_instruction_prompt(question: str, answer: Optional[str] = None) -> Tuple[str, Optional[str]]:\n",
    "    \"\"\"Create instruction-following prompts for FLAN-T5.\"\"\"\n",
    "    \n",
    "    # Instruction templates for variety\n",
    "    templates = [\n",
    "        \"Answer this hydroponic farming question: {question}\",\n",
    "        \"As a hydroponic farming expert, please answer: {question}\", \n",
    "        \"Provide guidance for this hydroponic farming query: {question}\",\n",
    "        \"Help with this hydroponic farming question: {question}\",\n",
    "        \"Give advice for hydroponic farming: {question}\"\n",
    "    ]\n",
    "    \n",
    "    # Select template based on question type\n",
    "    question_lower = question.lower()\n",
    "    if any(word in question_lower for word in ['how', 'what', 'why', 'when']):\n",
    "        template = templates[0]  # Direct Q&A\n",
    "    elif any(word in question_lower for word in ['help', 'advice']):\n",
    "        template = templates[4]  # Advice\n",
    "    else:\n",
    "        template = templates[1]  # Expert response\n",
    "    \n",
    "    input_text = template.format(question=question)\n",
    "    \n",
    "    return (input_text, answer) if answer is not None else input_text\n",
    "\n",
    "def process_dataset(df: pd.DataFrame) -> Dict[str, List[str]]:\n",
    "    \"\"\"Process and clean the dataset for training.\"\"\"\n",
    "    print(\"Cleaning data...\")\n",
    "    \n",
    "    # Clean text fields\n",
    "    df_clean = df.copy()\n",
    "    df_clean['question'] = df_clean['question'].apply(clean_text)\n",
    "    df_clean['answer'] = df_clean['answer'].apply(clean_text)\n",
    "    \n",
    "    # Filter out short or empty entries\n",
    "    min_length = 10\n",
    "    df_clean = df_clean[\n",
    "        (df_clean['question'].str.len() > min_length) & \n",
    "        (df_clean['answer'].str.len() > min_length)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Filtered dataset: {len(df_clean):,} samples (removed {len(df) - len(df_clean):,})\")\n",
    "    \n",
    "    # Create instruction-formatted pairs\n",
    "    instructions = []\n",
    "    targets = []\n",
    "    \n",
    "    for _, row in df_clean.iterrows():\n",
    "        instruction, target = create_instruction_prompt(row['question'], row['answer'])\n",
    "        instructions.append(instruction)\n",
    "        targets.append(target)\n",
    "    \n",
    "    return {\n",
    "        'input_text': instructions,\n",
    "        'target_text': targets\n",
    "    }\n",
    "\n",
    "# Process the dataset\n",
    "dataset_dict = process_dataset(df)\n",
    "instructions = dataset_dict['input_text']\n",
    "targets = dataset_dict['target_text']\n",
    "\n",
    "print(f\"Created {len(instructions):,} instruction-target pairs\")\n",
    "\n",
    "# Display sample and statistics\n",
    "print(f\"\\nSample Instruction:\")\n",
    "print(f\"   Input: {instructions[0]}\")\n",
    "print(f\"   Target: {targets[0]}\")\n",
    "\n",
    "# Calculate statistics\n",
    "avg_input_length = np.mean([len(text.split()) for text in instructions])\n",
    "avg_target_length = np.mean([len(text.split()) for text in targets])\n",
    "\n",
    "print(f\"\\nLength Statistics:\")\n",
    "print(f\"   Average input: {avg_input_length:.1f} words\")\n",
    "print(f\"   Average target: {avg_target_length:.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679862ce",
   "metadata": {},
   "source": [
    "## 5. Dataset Creation and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74ed887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Splits:\n",
      "   Training: 437 samples (69.9%)\n",
      "   Validation: 94 samples (15.0%)\n",
      "   Test: 94 samples (15.0%)\n",
      "   Total: 625 samples\n",
      "\n",
      "Datasets created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Dataset Creation and Train/Val/Test Split\n",
    "def create_datasets(instructions: List[str], targets: List[str], \n",
    "                   test_size: float = 0.3, val_size: float = 0.5, \n",
    "                   random_state: int = 42) -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    \"\"\"Create train, validation, and test datasets.\"\"\"\n",
    "    \n",
    "    # Split into train and temp (val + test)\n",
    "    train_inputs, temp_inputs, train_targets, temp_targets = train_test_split(\n",
    "        instructions, targets, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Split temp into validation and test\n",
    "    val_inputs, test_inputs, val_targets, test_targets = train_test_split(\n",
    "        temp_inputs, temp_targets, test_size=val_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Create HuggingFace datasets\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        'input_text': train_inputs,\n",
    "        'target_text': train_targets\n",
    "    })\n",
    "    \n",
    "    val_dataset = Dataset.from_dict({\n",
    "        'input_text': val_inputs,\n",
    "        'target_text': val_targets\n",
    "    })\n",
    "    \n",
    "    test_dataset = Dataset.from_dict({\n",
    "        'input_text': test_inputs,\n",
    "        'target_text': test_targets\n",
    "    })\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Create datasets\n",
    "train_dataset, val_dataset, test_dataset = create_datasets(instructions, targets)\n",
    "\n",
    "print(f\"Dataset Splits:\")\n",
    "print(f\"   Training: {len(train_dataset):,} samples ({len(train_dataset)/len(instructions)*100:.1f}%)\")\n",
    "print(f\"   Validation: {len(val_dataset):,} samples ({len(val_dataset)/len(instructions)*100:.1f}%)\")\n",
    "print(f\"   Test: {len(test_dataset):,} samples ({len(test_dataset)/len(instructions)*100:.1f}%)\")\n",
    "print(f\"   Total: {len(instructions):,} samples\")\n",
    "\n",
    "print(f\"\\nDatasets created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6d793d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing training data: 100%|██████████| 437/437 [00:00<00:00, 2172.38 examples/s]\n",
      "Tokenizing validation data:   0%|          | 0/94 [00:00<?, ? examples/s]\n",
      "Tokenizing validation data: 100%|██████████| 94/94 [00:00<00:00, 1903.57 examples/s]\n",
      "Tokenizing test data:   0%|          | 0/94 [00:00<?, ? examples/s]\n",
      "Tokenizing test data: 100%|██████████| 94/94 [00:00<00:00, 1761.16 examples/s]\n",
      "Tokenizing test data: 100%|██████████| 94/94 [00:00<00:00, 1761.16 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization completed!\n",
      "\n",
      "Tokenized Dataset Info:\n",
      "   Columns: ['input_ids', 'attention_mask', 'labels']\n",
      "   Features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': List(Value('int64'))}\n",
      "Tokenization Validation (Sample 0):\n",
      "   Input tokens: 20\n",
      "   Label tokens: 23\n",
      "   Decoded input: Answer this hydroponic farming question: How do I sanitize between crops?...\n",
      "   Decoded label: Drain scrub run 3% hydrogen peroxide or diluted bleach through lines then flush thoroughly with clean water.\n",
      "\n",
      "Data integrity verified!\n"
     ]
    }
   ],
   "source": [
    "# Dataset Tokenization\n",
    "# Tokenization parameters\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 256\n",
    "\n",
    "def tokenize_function(examples: Dict) -> Dict:\n",
    "    \"\"\"Tokenize inputs and targets for T5 model.\"\"\"\n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        examples['input_text'],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=False  # Data collator handles padding\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples['target_text'],\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            truncation=True,\n",
    "            padding=False\n",
    "        )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def validate_tokenization(dataset: Dataset, sample_idx: int = 0) -> None:\n",
    "    \"\"\"Validate tokenization results.\"\"\"\n",
    "    sample = dataset[sample_idx]\n",
    "    \n",
    "    # Decode sample for verification\n",
    "    input_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "    label_text = tokenizer.decode(sample['labels'], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Tokenization Validation (Sample {sample_idx}):\")\n",
    "    print(f\"   Input tokens: {len(sample['input_ids'])}\")\n",
    "    print(f\"   Label tokens: {len(sample['labels'])}\")\n",
    "    print(f\"   Decoded input: {input_text[:100]}...\")\n",
    "    print(f\"   Decoded label: {label_text}\")\n",
    "\n",
    "# Apply tokenization\n",
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing training data\"\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    desc=\"Tokenizing validation data\"\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names,\n",
    "    desc=\"Tokenizing test data\"\n",
    ")\n",
    "\n",
    "print(f\"Tokenization completed!\")\n",
    "\n",
    "# Validation\n",
    "print(f\"\\nTokenized Dataset Info:\")\n",
    "print(f\"   Columns: {train_dataset.column_names}\")\n",
    "print(f\"   Features: {train_dataset.features}\")\n",
    "\n",
    "validate_tokenization(train_dataset)\n",
    "\n",
    "# Verify data integrity\n",
    "assert 'input_text' not in train_dataset.column_names, \"Text columns should be removed\"\n",
    "assert 'target_text' not in train_dataset.column_names, \"Text columns should be removed\"\n",
    "print(f\"\\nData integrity verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57780d88",
   "metadata": {},
   "source": [
    "## 6. Fine-tuning Setup and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b5da6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup completed!\n",
      "\n",
      "Configuration Summary:\n",
      "   Device: cpu\n",
      "   Epochs: 12\n",
      "   Learning rate: 1e-05\n",
      "   Batch size: 2\n",
      "   Gradient accumulation: 4\n",
      "   Effective batch size: 8\n",
      "   Mixed precision: False\n",
      "\n",
      "Creating trainer...\n",
      "Training Data Summary:\n",
      "   Training samples: 437\n",
      "   Validation samples: 94\n",
      "   Expected time: ~90-120 minutes\n",
      "\n",
      "Performance Targets:\n",
      "   Training loss: < 2.0\n",
      "   ROUGE-1: > 0.35\n",
      "   ROUGE-2: > 0.08\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='660' max='660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [660/660 2:27:16, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.925200</td>\n",
       "      <td>4.478503</td>\n",
       "      <td>0.095112</td>\n",
       "      <td>0.004523</td>\n",
       "      <td>0.081460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.679600</td>\n",
       "      <td>4.269139</td>\n",
       "      <td>0.095132</td>\n",
       "      <td>0.003931</td>\n",
       "      <td>0.084614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.460000</td>\n",
       "      <td>4.113316</td>\n",
       "      <td>0.105809</td>\n",
       "      <td>0.005112</td>\n",
       "      <td>0.092320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.223900</td>\n",
       "      <td>3.995222</td>\n",
       "      <td>0.106279</td>\n",
       "      <td>0.004976</td>\n",
       "      <td>0.087887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.158800</td>\n",
       "      <td>3.922315</td>\n",
       "      <td>0.115303</td>\n",
       "      <td>0.008225</td>\n",
       "      <td>0.095984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.056100</td>\n",
       "      <td>3.857652</td>\n",
       "      <td>0.129753</td>\n",
       "      <td>0.010728</td>\n",
       "      <td>0.105896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.093000</td>\n",
       "      <td>3.816028</td>\n",
       "      <td>0.125963</td>\n",
       "      <td>0.008449</td>\n",
       "      <td>0.105601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.945600</td>\n",
       "      <td>3.774239</td>\n",
       "      <td>0.128265</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>0.104876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>4.046600</td>\n",
       "      <td>3.769816</td>\n",
       "      <td>0.131428</td>\n",
       "      <td>0.007591</td>\n",
       "      <td>0.111602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.893900</td>\n",
       "      <td>3.734317</td>\n",
       "      <td>0.135354</td>\n",
       "      <td>0.010398</td>\n",
       "      <td>0.115291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.886000</td>\n",
       "      <td>3.723239</td>\n",
       "      <td>0.143640</td>\n",
       "      <td>0.009256</td>\n",
       "      <td>0.117666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.850900</td>\n",
       "      <td>3.724364</td>\n",
       "      <td>0.127211</td>\n",
       "      <td>0.007290</td>\n",
       "      <td>0.102714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.920500</td>\n",
       "      <td>3.715766</td>\n",
       "      <td>0.141918</td>\n",
       "      <td>0.009120</td>\n",
       "      <td>0.114259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully!\n",
      "Final training loss: 4.1787\n",
      "Training loss still high - consider more epochs\n",
      "\n",
      "Training phase completed!\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning Setup and Training\n",
    "import os\n",
    "from typing import Union, Tuple, Optional\n",
    "\n",
    "# Disable wandb reporting (set environment variables only)\n",
    "os.environ.update({\n",
    "    \"WANDB_SILENT\": \"true\",\n",
    "    \"WANDB_DISABLED\": \"true\",\n",
    "    \"WANDB_MODE\": \"disabled\"\n",
    "})\n",
    "\n",
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    \"epochs\": 12,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"batch_size\": 4 if device.type == 'cuda' else 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_steps\": 100,\n",
    "    \"eval_steps\": 50,\n",
    "    \"save_steps\": 100,\n",
    "    \"logging_steps\": 25\n",
    "}\n",
    "\n",
    "# Generation configuration for enhanced responses\n",
    "GENERATION_CONFIG = {\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"min_length\": 25,\n",
    "    \"num_beams\": 6,\n",
    "    \"early_stopping\": True,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.85,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"repetition_penalty\": 1.3,\n",
    "    \"length_penalty\": 1.2,\n",
    "    \"diversity_penalty\": 0.2\n",
    "}\n",
    "\n",
    "def clean_response_text(response: str) -> str:\n",
    "    \"\"\"Clean generated response text.\"\"\"\n",
    "    response = response.strip()\n",
    "    # Remove repetitive patterns\n",
    "    response = re.sub(r'\\b(\\w+(?:\\s+\\w+){0,3})\\s*;\\s*\\1(?:\\s*;\\s*\\1)*', r'\\1', response)\n",
    "    response = re.sub(r'\\b(\\w+(?:\\s+\\w+){0,2})\\s+\\1\\b.*', r'\\1', response)\n",
    "    response = re.sub(r';+', ';', response)\n",
    "    response = re.sub(r'\\s+', ' ', response)\n",
    "    return response\n",
    "\n",
    "def compute_metrics(eval_pred) -> Dict[str, float]:\n",
    "    \"\"\"Compute ROUGE metrics for evaluation.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    \n",
    "    if not isinstance(predictions, np.ndarray):\n",
    "        predictions = np.array(predictions)\n",
    "    \n",
    "    if predictions.ndim == 3:\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    vocab_size = len(tokenizer)\n",
    "    predictions = np.clip(predictions, 0, vocab_size - 1)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    try:\n",
    "        decoded_preds = []\n",
    "        decoded_labels = []\n",
    "        \n",
    "        for pred_seq, label_seq in zip(predictions, labels):\n",
    "            # Filter valid tokens\n",
    "            valid_pred_tokens = [token for token in pred_seq if 0 <= token < vocab_size]\n",
    "            valid_label_tokens = [token for token in label_seq if 0 <= token < vocab_size]\n",
    "            \n",
    "            try:\n",
    "                pred_text = tokenizer.decode(valid_pred_tokens, skip_special_tokens=True)\n",
    "                label_text = tokenizer.decode(valid_label_tokens, skip_special_tokens=True)\n",
    "                decoded_preds.append(pred_text.strip())\n",
    "                decoded_labels.append(label_text.strip())\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to decode sequence: {e}\")\n",
    "                decoded_preds.append(\"no answer\")\n",
    "                decoded_labels.append(\"no answer\")\n",
    "        \n",
    "        # Handle empty predictions\n",
    "        decoded_preds = [pred if pred else \"no answer\" for pred in decoded_preds]\n",
    "        decoded_labels = [label if label else \"no answer\" for label in decoded_labels]\n",
    "        \n",
    "        # Compute ROUGE scores\n",
    "        result = rouge.compute(\n",
    "            predictions=decoded_preds,\n",
    "            references=decoded_labels,\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"rouge1\": result[\"rouge1\"],\n",
    "            \"rouge2\": result[\"rouge2\"],\n",
    "            \"rougeL\": result[\"rougeL\"]\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Metrics computation failed: {e}\")\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "\n",
    "class AdvancedT5Trainer(Trainer):\n",
    "    \"\"\"Enhanced T5 Trainer with improved generation capabilities.\"\"\"\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only: bool, ignore_keys=None):\n",
    "        \"\"\"Enhanced prediction step with better generation settings.\"\"\"\n",
    "        if prediction_loss_only:\n",
    "            return super().prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
    "        \n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs.get(\"attention_mask\", None)\n",
    "        labels = inputs.get(\"labels\", None)\n",
    "        \n",
    "        # Enhanced generation config\n",
    "        eval_config = GENERATION_CONFIG.copy()\n",
    "        tokenizer_ref = self.processing_class or self.tokenizer\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pad_token_id=tokenizer_ref.pad_token_id,\n",
    "                eos_token_id=tokenizer_ref.eos_token_id,\n",
    "                bos_token_id=getattr(tokenizer_ref, 'bos_token_id', None),\n",
    "                **eval_config\n",
    "            )\n",
    "        \n",
    "        # Ensure valid token range\n",
    "        vocab_size = len(tokenizer_ref)\n",
    "        generated_tokens = torch.clamp(generated_tokens, 0, vocab_size - 1)\n",
    "        \n",
    "        # Compute loss if needed\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "        \n",
    "        return (loss, generated_tokens, labels)\n",
    "\n",
    "# Setup training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(MODEL_DIR / \"flan-t5-hydroponic-checkpoints\"),\n",
    "    num_train_epochs=TRAINING_CONFIG[\"epochs\"],\n",
    "    per_device_train_batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=TRAINING_CONFIG[\"gradient_accumulation_steps\"],\n",
    "    warmup_steps=TRAINING_CONFIG[\"warmup_steps\"],\n",
    "    learning_rate=TRAINING_CONFIG[\"learning_rate\"],\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=str(MODEL_DIR / \"logs\"),\n",
    "    logging_steps=TRAINING_CONFIG[\"logging_steps\"],\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=TRAINING_CONFIG[\"eval_steps\"],\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=TRAINING_CONFIG[\"save_steps\"],\n",
    "    save_total_limit=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=device.type == 'cuda',\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    group_by_length=True\n",
    ")\n",
    "\n",
    "# Create data collator and load evaluation metric\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "print(\"Training setup completed!\")\n",
    "print(f\"\\nConfiguration Summary:\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Epochs: {TRAINING_CONFIG['epochs']}\")\n",
    "print(f\"   Learning rate: {TRAINING_CONFIG['learning_rate']}\")\n",
    "print(f\"   Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
    "print(f\"   Gradient accumulation: {TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"   Effective batch size: {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"   Mixed precision: {training_args.fp16}\")\n",
    "\n",
    "# Create trainer\n",
    "print(f\"\\nCreating trainer...\")\n",
    "trainer = AdvancedT5Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(f\"Training Data Summary:\")\n",
    "print(f\"   Training samples: {len(train_dataset):,}\")\n",
    "print(f\"   Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"   Expected time: ~90-120 minutes\")\n",
    "\n",
    "print(f\"\\nPerformance Targets:\")\n",
    "print(f\"   Training loss: < 2.0\")\n",
    "print(f\"   ROUGE-1: > 0.35\")\n",
    "print(f\"   ROUGE-2: > 0.08\")\n",
    "\n",
    "# Start training\n",
    "try:\n",
    "    print(f\"\\nStarting training...\")\n",
    "    training_output = trainer.train()\n",
    "    \n",
    "    print(f\"Training completed successfully!\")\n",
    "    final_loss = training_output.training_loss\n",
    "    print(f\"Final training loss: {final_loss:.4f}\")\n",
    "    \n",
    "    # Performance assessment\n",
    "    if final_loss < 2.0:\n",
    "        print(f\"EXCELLENT! Target achieved!\")\n",
    "    elif final_loss < 3.0:\n",
    "        print(f\"GOOD! Solid progress made!\")\n",
    "    else:\n",
    "        print(f\"Training loss still high - consider more epochs\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    print(f\"Try reducing batch size if out of memory\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\nTraining phase completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9606e137",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc1e907d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [47/47 04:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "   eval_loss: 3.6324\n",
      "   eval_rouge1: 0.1667\n",
      "   eval_rouge2: 0.0162\n",
      "   eval_rougeL: 0.1333\n",
      "\n",
      "Advanced Question Testing:\n",
      "\n",
      "1. Q: What is the optimal pH range for hydroponic lettuce and why?\n",
      "   A: Phosphorylation ranges from 0.5–0.2 °C for chlorophyll and 1–2 g/mL for leafy greens.\n",
      "\n",
      "1. Q: What is the optimal pH range for hydroponic lettuce and why?\n",
      "   A: Phosphorylation ranges from 0.5–0.2 °C for chlorophyll and 1–2 g/mL for leafy greens.\n",
      "\n",
      "2. Q: How often should I change the nutrient solution and what factors affect this?\n",
      "   A: Change the nutrient solution at least once a week to maintain optimal levels of nutrients and avoid over-dosing.\n",
      "\n",
      "2. Q: How often should I change the nutrient solution and what factors affect this?\n",
      "   A: Change the nutrient solution at least once a week to maintain optimal levels of nutrients and avoid over-dosing.\n",
      "\n",
      "3. Q: What are the best vegetables for hydroponic farming in Rwanda considering climate?\n",
      "   A: Vegetables can be grown in a wide variety of climates, such as tropical and subtropical climates.\n",
      "\n",
      "3. Q: What are the best vegetables for hydroponic farming in Rwanda considering climate?\n",
      "   A: Vegetables can be grown in a wide variety of climates, such as tropical and subtropical climates.\n",
      "\n",
      "4. Q: How do I prevent and treat root rot in hydroponic systems effectively?\n",
      "   A: Use pesticides and insecticides to prevent root rot in hydroponic systems; use fungicides or insecticidal sprays to reduce the risk of infection.\n",
      "\n",
      "4. Q: How do I prevent and treat root rot in hydroponic systems effectively?\n",
      "   A: Use pesticides and insecticides to prevent root rot in hydroponic systems; use fungicides or insecticidal sprays to reduce the risk of infection.\n",
      "\n",
      "5. Q: What essential nutrients do hydroponic tomatoes need for maximum yield?\n",
      "   A: Tomatoes need nitrogen, potassium, phosphorus, calcium, and potassium for optimal growth and fruit quality.\n",
      "\n",
      "5. Q: What essential nutrients do hydroponic tomatoes need for maximum yield?\n",
      "   A: Tomatoes need nitrogen, potassium, phosphorus, calcium, and potassium for optimal growth and fruit quality.\n",
      "\n",
      "6. Q: What's the difference between DWC and NFT systems for beginners?\n",
      "   A: DWC and NFT systems can be used for a variety of purposes, depending on the type of plant you're growing.\n",
      "\n",
      "6. Q: What's the difference between DWC and NFT systems for beginners?\n",
      "   A: DWC and NFT systems can be used for a variety of purposes, depending on the type of plant you're growing.\n",
      "\n",
      "7. Q: How do I maintain proper EC levels in my hydroponic nutrient solution?\n",
      "   A: EC levels should be maintained in a pH range of 6.0–7.0, and ensure that the EC level is consistent with the pH value of the solution.\n",
      "\n",
      "Performance Analysis:\n",
      "   Training Loss: 4.1787 (NEEDS MORE TRAINING)\n",
      "   ROUGE-1: 0.1667 (NEEDS IMPROVEMENT)\n",
      "   ROUGE-2: 0.0162 (NEEDS IMPROVEMENT)\n",
      "   ROUGE-L: 0.1333\n",
      "\n",
      "Response Quality Analysis:\n",
      "\n",
      "7. Q: How do I maintain proper EC levels in my hydroponic nutrient solution?\n",
      "   A: EC levels should be maintained in a pH range of 6.0–7.0, and ensure that the EC level is consistent with the pH value of the solution.\n",
      "\n",
      "Performance Analysis:\n",
      "   Training Loss: 4.1787 (NEEDS MORE TRAINING)\n",
      "   ROUGE-1: 0.1667 (NEEDS IMPROVEMENT)\n",
      "   ROUGE-2: 0.0162 (NEEDS IMPROVEMENT)\n",
      "   ROUGE-L: 0.1333\n",
      "\n",
      "Response Quality Analysis:\n",
      "\n",
      "1. Q: What pH level should I maintain for hydroponic tomatoes?\n",
      "   A: Maintain a pH of 6.0–7.0, and maintain a level of 6.5–7.8 for tomatoes.\n",
      "   Quality: GOOD | Length: 13 | Complexity: 0.85 | Repetition: 0.15\n",
      "\n",
      "1. Q: What pH level should I maintain for hydroponic tomatoes?\n",
      "   A: Maintain a pH of 6.0–7.0, and maintain a level of 6.5–7.8 for tomatoes.\n",
      "   Quality: GOOD | Length: 13 | Complexity: 0.85 | Repetition: 0.15\n",
      "\n",
      "2. Q: How do I prevent algae growth in my hydroponic system?\n",
      "   A: Use a pH-balanced filter and use a high-quality filtration system to prevent algae growth in your system.\n",
      "   Quality: EXCELLENT | Length: 17 | Complexity: 0.94 | Repetition: 0.06\n",
      "\n",
      "2. Q: How do I prevent algae growth in my hydroponic system?\n",
      "   A: Use a pH-balanced filter and use a high-quality filtration system to prevent algae growth in your system.\n",
      "   Quality: EXCELLENT | Length: 17 | Complexity: 0.94 | Repetition: 0.06\n",
      "\n",
      "3. Q: What are the signs of nutrient deficiency in hydroponic plants?\n",
      "   A: Signs of nutrient deficiency include low pH, low oxygen levels, and a lack of nutrients.\n",
      "   Quality: GOOD | Length: 15 | Complexity: 0.87 | Repetition: 0.13\n",
      "\n",
      "3. Q: What are the signs of nutrient deficiency in hydroponic plants?\n",
      "   A: Signs of nutrient deficiency include low pH, low oxygen levels, and a lack of nutrients.\n",
      "   Quality: GOOD | Length: 15 | Complexity: 0.87 | Repetition: 0.13\n",
      "\n",
      "4. Q: How much light do hydroponic vegetables need daily?\n",
      "   A: Light varies depending on the type of plant, but can vary depending on temperature and light intensity. Make sure your plants are exposed to at least 20–30 hours of direct sunlight per day.\n",
      "   Quality: EXCELLENT | Length: 33 | Complexity: 0.91 | Repetition: 0.09\n",
      "\n",
      "4. Q: How much light do hydroponic vegetables need daily?\n",
      "   A: Light varies depending on the type of plant, but can vary depending on temperature and light intensity. Make sure your plants are exposed to at least 20–30 hours of direct sunlight per day.\n",
      "   Quality: EXCELLENT | Length: 33 | Complexity: 0.91 | Repetition: 0.09\n",
      "\n",
      "5. Q: What's the difference between DWC and NFT hydroponic systems?\n",
      "   A: DWC and NFT hydroponic systems are based on a combination of syringes and pumping systems.\n",
      "   Quality: GOOD | Length: 15 | Complexity: 0.93 | Repetition: 0.07\n",
      "\n",
      "5. Q: What's the difference between DWC and NFT hydroponic systems?\n",
      "   A: DWC and NFT hydroponic systems are based on a combination of syringes and pumping systems.\n",
      "   Quality: GOOD | Length: 15 | Complexity: 0.93 | Repetition: 0.07\n",
      "\n",
      "6. Q: How do I calculate the right nutrient concentration for lettuce?\n",
      "   A: Calculate nutrient concentrations using a pH value of 6.5–7.0 and a weighted average of 10–15 g/mL.\n",
      "   Quality: GOOD | Length: 16 | Complexity: 0.88 | Repetition: 0.12\n",
      "\n",
      "6. Q: How do I calculate the right nutrient concentration for lettuce?\n",
      "   A: Calculate nutrient concentrations using a pH value of 6.5–7.0 and a weighted average of 10–15 g/mL.\n",
      "   Quality: GOOD | Length: 16 | Complexity: 0.88 | Repetition: 0.12\n",
      "\n",
      "7. Q: What temperature should I maintain in my hydroponic greenhouse?\n",
      "   A: Temperatures should be between 68°F (77°C) and 82°F (20°C).\n",
      "   Quality: FAIR | Length: 9 | Complexity: 1.00 | Repetition: 0.00\n",
      "\n",
      "7. Q: What temperature should I maintain in my hydroponic greenhouse?\n",
      "   A: Temperatures should be between 68°F (77°C) and 82°F (20°C).\n",
      "   Quality: FAIR | Length: 9 | Complexity: 1.00 | Repetition: 0.00\n",
      "\n",
      "8. Q: Which crops are most profitable for hydroponic farming in Rwanda?\n",
      "   A: Agro-based crops such as soybeans and maize are the most profitable for hydroponic farming in Rwanda.\n",
      "   Quality: EXCELLENT | Length: 16 | Complexity: 1.00 | Repetition: 0.00\n",
      "\n",
      "Overall Quality Metrics:\n",
      "   Average Length: 16.8 words\n",
      "   Average Complexity: 0.92\n",
      "   Average Repetition: 0.08\n",
      "\n",
      "Final Assessment:\n",
      "   Overall Score: 50/100\n",
      "   Status: MODERATE QUALITY\n",
      "   Recommendation: Needs additional training or fine-tuning\n",
      "\n",
      "Next Steps:\n",
      "   - Continue training with lower learning rate\n",
      "   - Expand dataset with more examples\n",
      "   - Fine-tune generation parameters\n",
      "\n",
      "Evaluation completed!\n",
      "\n",
      "8. Q: Which crops are most profitable for hydroponic farming in Rwanda?\n",
      "   A: Agro-based crops such as soybeans and maize are the most profitable for hydroponic farming in Rwanda.\n",
      "   Quality: EXCELLENT | Length: 16 | Complexity: 1.00 | Repetition: 0.00\n",
      "\n",
      "Overall Quality Metrics:\n",
      "   Average Length: 16.8 words\n",
      "   Average Complexity: 0.92\n",
      "   Average Repetition: 0.08\n",
      "\n",
      "Final Assessment:\n",
      "   Overall Score: 50/100\n",
      "   Status: MODERATE QUALITY\n",
      "   Recommendation: Needs additional training or fine-tuning\n",
      "\n",
      "Next Steps:\n",
      "   - Continue training with lower learning rate\n",
      "   - Expand dataset with more examples\n",
      "   - Fine-tune generation parameters\n",
      "\n",
      "Evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Model Evaluation\n",
    "def generate_enhanced_response(question: str, model, tokenizer, \n",
    "                             config: Optional[Dict] = None) -> str:\n",
    "    \"\"\"Generate enhanced response with improved settings.\"\"\"\n",
    "    if config is None:\n",
    "        config = GENERATION_CONFIG\n",
    "    \n",
    "    input_text = f\"Answer this hydroponic farming question: {question}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    enhanced_config = config.copy()\n",
    "    enhanced_config.update({\n",
    "        \"max_new_tokens\": 120,\n",
    "        \"num_beams\": 6,\n",
    "        \"temperature\": 0.8,\n",
    "        \"repetition_penalty\": 1.3,\n",
    "        \"length_penalty\": 1.2\n",
    "    })\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            **enhanced_config,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return clean_response_text(response)\n",
    "\n",
    "def analyze_response_quality(response: str) -> Dict[str, Union[float, str, int]]:\n",
    "    \"\"\"Analyze response quality with multiple metrics.\"\"\"\n",
    "    words = response.split()\n",
    "    if not words:\n",
    "        return {\"repetition\": 1.0, \"quality\": \"Poor - Empty response\", \"length\": 0, \"complexity\": 0}\n",
    "    \n",
    "    unique_words = len(set(words))\n",
    "    total_words = len(words)\n",
    "    repetition_score = (total_words - unique_words) / total_words\n",
    "    complexity_score = unique_words / total_words\n",
    "    \n",
    "    # Quality assessment\n",
    "    if repetition_score < 0.1 and complexity_score > 0.7 and total_words > 15:\n",
    "        quality = \"EXCELLENT\"\n",
    "    elif repetition_score < 0.2 and complexity_score > 0.6 and total_words > 10:\n",
    "        quality = \"GOOD\"\n",
    "    elif repetition_score < 0.3 and total_words > 5:\n",
    "        quality = \"FAIR\"\n",
    "    else:\n",
    "        quality = \"POOR\"\n",
    "    \n",
    "    return {\n",
    "        \"repetition\": repetition_score,\n",
    "        \"quality\": quality,\n",
    "        \"length\": total_words,\n",
    "        \"complexity\": complexity_score\n",
    "    }\n",
    "\n",
    "def assess_model_performance(training_loss: float, rouge_scores: Dict[str, float]) -> Dict[str, str]:\n",
    "    \"\"\"Assess overall model performance.\"\"\"\n",
    "    loss_status = (\"EXCELLENT\" if training_loss < 2.0 else \n",
    "                  \"GOOD\" if training_loss < 3.0 else \"NEEDS MORE TRAINING\")\n",
    "    \n",
    "    rouge1_status = (\"EXCELLENT\" if rouge_scores['eval_rouge1'] > 0.35 else\n",
    "                    \"GOOD\" if rouge_scores['eval_rouge1'] > 0.25 else \"NEEDS IMPROVEMENT\")\n",
    "    \n",
    "    rouge2_status = (\"EXCELLENT\" if rouge_scores['eval_rouge2'] > 0.08 else\n",
    "                    \"GOOD\" if rouge_scores['eval_rouge2'] > 0.05 else \"NEEDS IMPROVEMENT\")\n",
    "    \n",
    "    return {\n",
    "        \"loss_status\": loss_status,\n",
    "        \"rouge1_status\": rouge1_status,\n",
    "        \"rouge2_status\": rouge2_status\n",
    "    }\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "for key, value in test_results.items():\n",
    "    if 'rouge' in key or 'loss' in key:\n",
    "        print(f\"   {key}: {value:.4f}\")\n",
    "\n",
    "# Advanced test questions\n",
    "ADVANCED_QUESTIONS = [\n",
    "    \"What is the optimal pH range for hydroponic lettuce and why?\",\n",
    "    \"How often should I change the nutrient solution and what factors affect this?\",\n",
    "    \"What are the best vegetables for hydroponic farming in Rwanda considering climate?\",\n",
    "    \"How do I prevent and treat root rot in hydroponic systems effectively?\",\n",
    "    \"What essential nutrients do hydroponic tomatoes need for maximum yield?\",\n",
    "    \"What's the difference between DWC and NFT systems for beginners?\",\n",
    "    \"How do I maintain proper EC levels in my hydroponic nutrient solution?\"\n",
    "]\n",
    "\n",
    "print(f\"\\nAdvanced Question Testing:\")\n",
    "model.eval()\n",
    "\n",
    "for i, question in enumerate(ADVANCED_QUESTIONS, 1):\n",
    "    try:\n",
    "        response = generate_enhanced_response(question, model, tokenizer)\n",
    "        print(f\"\\n{i}. Q: {question}\")\n",
    "        print(f\"   A: {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{i}. Q: {question}\")\n",
    "        print(f\"   Error: {e}\")\n",
    "\n",
    "# Performance analysis\n",
    "performance = assess_model_performance(training_output.training_loss, test_results)\n",
    "\n",
    "print(f\"\\nPerformance Analysis:\")\n",
    "print(f\"   Training Loss: {training_output.training_loss:.4f} ({performance['loss_status']})\")\n",
    "print(f\"   ROUGE-1: {test_results['eval_rouge1']:.4f} ({performance['rouge1_status']})\")\n",
    "print(f\"   ROUGE-2: {test_results['eval_rouge2']:.4f} ({performance['rouge2_status']})\")\n",
    "print(f\"   ROUGE-L: {test_results['eval_rougeL']:.4f}\")\n",
    "\n",
    "# Comprehensive quality testing\n",
    "QUALITY_TEST_QUESTIONS = [\n",
    "    \"What pH level should I maintain for hydroponic tomatoes?\",\n",
    "    \"How do I prevent algae growth in my hydroponic system?\",\n",
    "    \"What are the signs of nutrient deficiency in hydroponic plants?\",\n",
    "    \"How much light do hydroponic vegetables need daily?\",\n",
    "    \"What's the difference between DWC and NFT hydroponic systems?\",\n",
    "    \"How do I calculate the right nutrient concentration for lettuce?\",\n",
    "    \"What temperature should I maintain in my hydroponic greenhouse?\",\n",
    "    \"Which crops are most profitable for hydroponic farming in Rwanda?\"\n",
    "]\n",
    "\n",
    "print(f\"\\nResponse Quality Analysis:\")\n",
    "quality_metrics = {\"repetition\": [], \"complexity\": [], \"length\": []}\n",
    "\n",
    "for i, question in enumerate(QUALITY_TEST_QUESTIONS, 1):\n",
    "    try:\n",
    "        response = generate_enhanced_response(question, model, tokenizer)\n",
    "        analysis = analyze_response_quality(response)\n",
    "        \n",
    "        quality_metrics[\"repetition\"].append(analysis[\"repetition\"])\n",
    "        quality_metrics[\"complexity\"].append(analysis[\"complexity\"])\n",
    "        quality_metrics[\"length\"].append(analysis[\"length\"])\n",
    "        \n",
    "        print(f\"\\n{i}. Q: {question}\")\n",
    "        print(f\"   A: {response}\")\n",
    "        print(f\"   Quality: {analysis['quality']} | Length: {analysis['length']} | \"\n",
    "              f\"Complexity: {analysis['complexity']:.2f} | Repetition: {analysis['repetition']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{i}. Error with question: {e}\")\n",
    "\n",
    "# Final assessment\n",
    "if quality_metrics[\"repetition\"]:\n",
    "    avg_repetition = np.mean(quality_metrics[\"repetition\"])\n",
    "    avg_complexity = np.mean(quality_metrics[\"complexity\"])\n",
    "    avg_length = np.mean(quality_metrics[\"length\"])\n",
    "    \n",
    "    print(f\"\\nOverall Quality Metrics:\")\n",
    "    print(f\"   Average Length: {avg_length:.1f} words\")\n",
    "    print(f\"   Average Complexity: {avg_complexity:.2f}\")\n",
    "    print(f\"   Average Repetition: {avg_repetition:.2f}\")\n",
    "    \n",
    "    # Calculate performance score\n",
    "    performance_score = 0\n",
    "    if training_output.training_loss < 2.0:\n",
    "        performance_score += 25\n",
    "    elif training_output.training_loss < 3.0:\n",
    "        performance_score += 15\n",
    "    \n",
    "    if test_results['eval_rouge1'] > 0.35:\n",
    "        performance_score += 25\n",
    "    elif test_results['eval_rouge1'] > 0.25:\n",
    "        performance_score += 15\n",
    "    \n",
    "    if avg_repetition < 0.2:\n",
    "        performance_score += 25\n",
    "    elif avg_repetition < 0.3:\n",
    "        performance_score += 15\n",
    "    \n",
    "    if avg_complexity > 0.7:\n",
    "        performance_score += 25\n",
    "    elif avg_complexity > 0.6:\n",
    "        performance_score += 15\n",
    "    \n",
    "    print(f\"\\nFinal Assessment:\")\n",
    "    print(f\"   Overall Score: {performance_score}/100\")\n",
    "    \n",
    "    if performance_score >= 80:\n",
    "        status = \"PRODUCTION READY\"\n",
    "        recommendation = \"Deploy immediately with confidence\"\n",
    "    elif performance_score >= 60:\n",
    "        status = \"GOOD QUALITY\"\n",
    "        recommendation = \"Suitable for testing and gradual deployment\"\n",
    "    elif performance_score >= 40:\n",
    "        status = \"MODERATE QUALITY\"\n",
    "        recommendation = \"Needs additional training or fine-tuning\"\n",
    "    else:\n",
    "        status = \"NEEDS IMPROVEMENT\"\n",
    "        recommendation = \"Requires significant improvements\"\n",
    "    \n",
    "    print(f\"   Status: {status}\")\n",
    "    print(f\"   Recommendation: {recommendation}\")\n",
    "    \n",
    "    print(f\"\\nNext Steps:\")\n",
    "    if performance_score >= 70:\n",
    "        print(f\"   - Save model and integrate with app.py\")\n",
    "        print(f\"   - Use enhanced generation settings in production\")\n",
    "        print(f\"   - Monitor user feedback and iterate\")\n",
    "    else:\n",
    "        print(f\"   - Continue training with lower learning rate\")\n",
    "        print(f\"   - Expand dataset with more examples\")\n",
    "        print(f\"   - Fine-tune generation parameters\")\n",
    "\n",
    "print(f\"\\nEvaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e07ee5",
   "metadata": {},
   "source": [
    "## 9. Save the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62ecdffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing memory...\n",
      "Saving fine-tuned model...\n",
      "SUCCESS: Fine-tuned model (final) saved to: c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\trained_model\\flan-t5-hydroponic-final\n",
      "SUCCESS: Fine-tuned model (final) saved to: c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\trained_model\\flan-t5-hydroponic-final\n",
      "SUCCESS: Fine-tuned model (app compatible) saved to: c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\trained_model\n",
      "Model info saved to: c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\trained_model\\model_info.json\n",
      "Generation config saved to: c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\trained_model\\generation_config.json\n",
      "\n",
      "Model Saving Summary:\n",
      "Model Locations:\n",
      "   Final model: c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\trained_model\\flan-t5-hydroponic-final\n",
      "   App-ready model: c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\trained_model\n",
      "\n",
      "Model Performance Summary:\n",
      "   Training Loss: 4.1787\n",
      "   Test ROUGE-1: 0.1667\n",
      "   Test ROUGE-2: 0.0162\n",
      "   Test ROUGE-L: 0.1333\n",
      "\n",
      "Ready for deployment!\n",
      "   Use the model in c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\trained_model for your application\n",
      "   Reference generation_config.json for optimal settings\n",
      "Memory cleaned and model saving completed!\n",
      "SUCCESS: Fine-tuned model (app compatible) saved to: c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\trained_model\n",
      "Model info saved to: c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\trained_model\\model_info.json\n",
      "Generation config saved to: c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\trained_model\\generation_config.json\n",
      "\n",
      "Model Saving Summary:\n",
      "Model Locations:\n",
      "   Final model: c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\trained_model\\flan-t5-hydroponic-final\n",
      "   App-ready model: c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\trained_model\n",
      "\n",
      "Model Performance Summary:\n",
      "   Training Loss: 4.1787\n",
      "   Test ROUGE-1: 0.1667\n",
      "   Test ROUGE-2: 0.0162\n",
      "   Test ROUGE-L: 0.1333\n",
      "\n",
      "Ready for deployment!\n",
      "   Use the model in c:\\Users\\HP\\Desktop\\ALU\\Farmsmart_growmate_chatbot\\trained_model for your application\n",
      "   Reference generation_config.json for optimal settings\n",
      "Memory cleaned and model saving completed!\n"
     ]
    }
   ],
   "source": [
    "# Save Fine-tuned Model\n",
    "import gc\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU and system memory.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def save_model_safely(model, tokenizer, save_path: Path, description: str) -> bool:\n",
    "    \"\"\"Save model and tokenizer with error handling.\"\"\"\n",
    "    try:\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save with safe_serialization=False for Windows compatibility\n",
    "        model.save_pretrained(save_path, safe_serialization=False)\n",
    "        tokenizer.save_pretrained(save_path)\n",
    "        \n",
    "        print(f\"SUCCESS: {description} saved to: {save_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to save {description}: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_model_info(model_name: str, training_config: Dict, \n",
    "                     training_results: Dict, test_results: Dict) -> Dict:\n",
    "    \"\"\"Create comprehensive model information.\"\"\"\n",
    "    return {\n",
    "        \"model_info\": {\n",
    "            \"base_model\": model_name,\n",
    "            \"model_type\": \"FLAN-T5-base fine-tuned for hydroponic farming\",\n",
    "            \"creation_date\": datetime.now().isoformat(),\n",
    "            \"pytorch_version\": torch.__version__\n",
    "        },\n",
    "        \"dataset_info\": {\n",
    "            \"training_samples\": len(train_dataset),\n",
    "            \"validation_samples\": len(val_dataset),\n",
    "            \"test_samples\": len(test_dataset),\n",
    "            \"max_input_length\": MAX_INPUT_LENGTH,\n",
    "            \"max_target_length\": MAX_TARGET_LENGTH\n",
    "        },\n",
    "        \"training_config\": training_config,\n",
    "        \"performance_metrics\": {\n",
    "            \"final_training_loss\": training_results.training_loss,\n",
    "            \"test_rouge1\": test_results['eval_rouge1'],\n",
    "            \"test_rouge2\": test_results['eval_rouge2'],\n",
    "            \"test_rougeL\": test_results['eval_rougeL'],\n",
    "            \"test_loss\": test_results['eval_loss']\n",
    "        },\n",
    "        \"generation_config\": GENERATION_CONFIG,\n",
    "        \"usage_instructions\": {\n",
    "            \"input_format\": \"Answer this hydroponic farming question: {question}\",\n",
    "            \"recommended_max_length\": 512,\n",
    "            \"recommended_generation_config\": GENERATION_CONFIG\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Clear memory before saving\n",
    "print(\"Clearing memory...\")\n",
    "clear_memory()\n",
    "\n",
    "# Define save paths\n",
    "final_model_path = MODEL_DIR / \"flan-t5-hydroponic-final\"\n",
    "main_model_path = BASE_DIR / \"trained_model\"\n",
    "\n",
    "print(f\"Saving fine-tuned model...\")\n",
    "\n",
    "# Save to final model directory\n",
    "success_final = save_model_safely(\n",
    "    model, tokenizer, final_model_path, \n",
    "    \"Fine-tuned model (final)\"\n",
    ")\n",
    "\n",
    "# Save to main directory for app.py compatibility\n",
    "success_main = save_model_safely(\n",
    "    model, tokenizer, main_model_path,\n",
    "    \"Fine-tuned model (app compatible)\"\n",
    ")\n",
    "\n",
    "# Create and save model information\n",
    "if success_main:\n",
    "    try:\n",
    "        model_info = create_model_info(\n",
    "            MODEL_NAME, TRAINING_CONFIG, \n",
    "            training_output, test_results\n",
    "        )\n",
    "        \n",
    "        info_file = main_model_path / \"model_info.json\"\n",
    "        with open(info_file, \"w\", encoding='utf-8') as f:\n",
    "            json.dump(model_info, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Model info saved to: {info_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not save model info: {e}\")\n",
    "\n",
    "# Save generation config separately for easy access\n",
    "try:\n",
    "    config_file = main_model_path / \"generation_config.json\"\n",
    "    with open(config_file, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(GENERATION_CONFIG, f, indent=2)\n",
    "    \n",
    "    print(f\"Generation config saved to: {config_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not save generation config: {e}\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nModel Saving Summary:\")\n",
    "print(f\"Model Locations:\")\n",
    "if success_final:\n",
    "    print(f\"   Final model: {final_model_path}\")\n",
    "if success_main:\n",
    "    print(f\"   App-ready model: {main_model_path}\")\n",
    "\n",
    "print(f\"\\nModel Performance Summary:\")\n",
    "print(f\"   Training Loss: {training_output.training_loss:.4f}\")\n",
    "print(f\"   Test ROUGE-1: {test_results['eval_rouge1']:.4f}\")\n",
    "print(f\"   Test ROUGE-2: {test_results['eval_rouge2']:.4f}\")\n",
    "print(f\"   Test ROUGE-L: {test_results['eval_rougeL']:.4f}\")\n",
    "\n",
    "print(f\"\\nReady for deployment!\")\n",
    "print(f\"   Use the model in {main_model_path} for your application\")\n",
    "print(f\"   Reference generation_config.json for optimal settings\")\n",
    "\n",
    "# Clean up memory one more time\n",
    "clear_memory()\n",
    "print(f\"Memory cleaned and model saving completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
